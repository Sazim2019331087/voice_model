{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sazim2019331087/voice_model/blob/main/CNN_RNN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCEbcmHA6eIY"
      },
      "outputs": [],
      "source": [
        "# --- IMPORTANT WARNING ---\n",
        "print(\"=\"*80)\n",
        "print(\"WARNING: Training a deep learning model from scratch (without pre-training)\")\n",
        "print(\"         with only 147 audio files is highly challenging and prone to overfitting.\")\n",
        "print(\"         The model will likely have limited generalization to new voices not in your dataset.\")\n",
        "print(\"         For a robust, good-performing model, you typically need thousands of hours of audio data.\")\n",
        "print(\"         This code is provided for educational purposes to demonstrate the architecture.\")\n",
        "print(\"         Ensure you have a GPU runtime enabled in Colab, as CPU training will be extremely slow.\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Installing required libraries ---\")\n",
        "!pip install --upgrade pip\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Install PyTorch with CUDA support\n",
        "!pip install pandas scikit-learn joblib tqdm # tqdm for progress bars\n",
        "!pip install ffmpeg-python # Python wrapper for ffmpeg\n",
        "!apt-get update && apt-get install -y ffmpeg # Install ffmpeg on Colab for audio processing"
      ],
      "metadata": {
        "id": "uVN-fQ7P6miM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# --- Step 1: Setup and Install Libraries ---\n",
        "# ==============================================================================\n",
        "# You need to run this cell before any other code.\n",
        "\n",
        "print(\"--- Installing required libraries ---\")\n",
        "!pip install --upgrade pip\n",
        "!pip install pandas scikit-learn joblib tqdm # pandas, scikit-learn, joblib, tqdm\n",
        "!pip install ffmpeg-python # Python wrapper for ffmpeg\n",
        "!apt-get update && apt-get install -y ffmpeg # Install ffmpeg on Colab for audio processing\n",
        "\n",
        "# --- CRITICAL: Re-install PyTorch and TorchAudio to ensure CUDA version compatibility ---\n",
        "# This line will install PyTorch and a compatible torchaudio/torchvision\n",
        "# It's important to specify the PyTorch version first, then let torchaudio/torchvision follow.\n",
        "# This command will usually select the correct torchaudio version automatically.\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "# We specifically target torch==2.0.1 and torchaudio==2.0.2 for cu118,\n",
        "# which are known to be compatible based on common PyTorch versions.\n",
        "\n",
        "# After this, restart the runtime as before.\n",
        "\n",
        "print(\"Installation complete. Please RESTART YOUR COLAB RUNTIME (Runtime -> Restart session) and then run all cells from the beginning.\")"
      ],
      "metadata": {
        "id": "Pr5UvYib8EQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Mount Google Drive and Load Data\n",
        "# ==============================================================================\n",
        "# This step connects your Colab notebook to your Google Drive to access the data.\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchaudio\n",
        "from torchaudio.transforms import MFCC # For MFCC feature extraction\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm # For progress bars\n",
        "import joblib # For saving and loading the speaker mapping and model\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"\\n--- Mounting Google Drive ---\")\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jUaCIexb7Wh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define the paths to your data ---\n",
        "# IMPORTANT: Make sure these paths match your Google Drive structure exactly.\n",
        "# Example: If your main folder is 'project' in My Drive and audio files are in 'voices' subfolder.\n",
        "PROJECT_ROOT_DIR = '/content/drive/MyDrive/project'\n",
        "CSV_PATH = os.path.join(PROJECT_ROOT_DIR, 'training.csv')\n",
        "AUDIO_FOLDER_PATH = os.path.join(PROJECT_ROOT_DIR, 'voices')\n",
        "\n",
        "# Check if the directories and files exist\n",
        "if not os.path.exists(PROJECT_ROOT_DIR):\n",
        "    raise FileNotFoundError(f\"Error: Project folder '{PROJECT_ROOT_DIR}' not found. Please check the path and your Google Drive structure.\")\n",
        "elif not os.path.exists(CSV_PATH):\n",
        "    raise FileNotFoundError(f\"Error: CSV file '{CSV_PATH}' not found. Please ensure it's in the correct location.\")\n",
        "elif not os.path.exists(AUDIO_FOLDER_PATH):\n",
        "    raise FileNotFoundError(f\"Error: Audio folder '{AUDIO_FOLDER_PATH}' not found. Please check the path and upload your audio files.\")\n",
        "else:\n",
        "    print(f\"Successfully located project folder at: {PROJECT_ROOT_DIR}\")\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "print(\"\\n--- Loading data from CSV ---\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Construct the full path to each audio file\n",
        "df['audio_path'] = df['audio_file'].apply(lambda x: os.path.join(AUDIO_FOLDER_PATH, x))\n",
        "\n",
        "# --- IMPORTANT: Filter out missing/unreadable audio files ---\n",
        "# This step is crucial to prevent errors during audio loading.\n",
        "print(\"\\n--- Verifying audio file paths and formats... ---\")\n",
        "verified_data_for_df = [] # To store valid rows for the new DataFrame\n",
        "\n",
        "# Create a temporary MFCC transform to test audio loading and feature shape\n",
        "test_mfcc_transform = MFCC(\n",
        "    sample_rate=16000,\n",
        "    n_mfcc=40, # Assuming 40 MFCCs\n",
        "    melkwargs={\n",
        "        'n_fft': 400,\n",
        "        'hop_length': 160,\n",
        "        'n_mels': 128\n",
        "    }\n",
        ")\n",
        "\n",
        "problematic_files = []\n",
        "\n",
        "# Iterate through each row to verify audio files\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Verifying audio files\"):\n",
        "    audio_file_path = row['audio_path']\n",
        "\n",
        "    if not os.path.exists(audio_file_path):\n",
        "        problematic_files.append((row['audio_file'], row['email'], \"File Not Found\"))\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        waveform, sample_rate = torchaudio.load(audio_file_path, frame_offset=0, num_frames=16000 * 2) # Load first 2 seconds\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            waveform = resampler(waveform)\n",
        "        if waveform.shape[0] > 1: # Convert to mono\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "        # Ensure waveform is 2D (channels, samples)\n",
        "        elif waveform.ndim == 1:\n",
        "            waveform = waveform.unsqueeze(0) # Add channel dimension if it was (samples,)\n",
        "\n",
        "        # Test MFCC transformation\n",
        "        temp_mfcc_features = test_mfcc_transform(waveform)\n",
        "\n",
        "        # Squeeze the channel dimension for the test if it's there\n",
        "        if temp_mfcc_features.ndim == 3 and temp_mfcc_features.shape[0] == 1:\n",
        "            temp_mfcc_features = temp_mfcc_features.squeeze(0)\n",
        "\n",
        "        # Verify it's 2D (num_mfcc, num_frames) after squeezing\n",
        "        if temp_mfcc_features.ndim != 2:\n",
        "            raise ValueError(f\"MFCC features for {audio_file_path} unexpected shape after squeeze: {temp_mfcc_features.shape}\")\n",
        "\n",
        "        # If successfully processed, add the original row to our verified list\n",
        "        verified_data_for_df.append(row.to_dict())\n",
        "\n",
        "    except Exception as e:\n",
        "        problematic_files.append((row['audio_file'], row['email'], f\"Format Error: {e}\"))\n",
        "\n",
        "if problematic_files:\n",
        "    print(f\"\\n--- {len(problematic_files)} Problematic audio files found and skipped ---\")\n",
        "    problematic_df = pd.DataFrame(problematic_files, columns=['audio_ffile', 'email', 'Reason'])\n",
        "    print(problematic_df.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "    print(\"\\nTip: Use `ffmpeg -i input.wav -ar 16000 -ac 1 -c:a pcm_s16le output_converted.wav` to convert problematic files.\")\n",
        "else:\n",
        "    print(\"\\nAll audio files verified successfully!\")\n",
        "\n",
        "# Create a new DataFrame with only verified files\n",
        "if not verified_data_for_df:\n",
        "    raise ValueError(\"No valid audio files found after verification. Please check your data.\")\n",
        "\n",
        "existing_files_df = pd.DataFrame(verified_data_for_df)\n",
        "existing_files_df['speaker_id'] = existing_files_df['email'].astype('category').cat.codes\n",
        "speaker_mapping = dict(enumerate(existing_files_df['email'].astype('category').cat.categories))\n",
        "num_speakers = len(speaker_mapping)\n",
        "\n",
        "print(f\"\\n--- Speaker Mapping (Total Unique Speakers: {num_speakers}) ---\")\n",
        "print(speaker_mapping)\n",
        "\n",
        "# --- CRITICAL FIX for ValueError in train_test_split ---\n",
        "# Split data into training and testing sets\n",
        "# We use test_size=num_speakers to ensure there's at least one sample per speaker in the test set.\n",
        "# This is required for stratification when the number of speakers is high and samples per speaker are low.\n",
        "if len(existing_files_df) < num_speakers * 2: # Check if there are enough samples for a reasonable split\n",
        "    print(\"\\nWARNING: Dataset has very few samples per speaker. Stratification might be difficult.\")\n",
        "    print(f\"Total samples: {len(existing_files_df)}, Unique speakers: {num_speakers}\")\n",
        "    # Fallback to a non-stratified split if stratification is impossible\n",
        "    # Or adjust test_size to be a fraction that might pass, but not ideal for evaluation\n",
        "    train_df, test_df = train_test_split(\n",
        "        existing_files_df,\n",
        "        test_size=max(1, min(int(0.2 * len(existing_files_df)), num_speakers)), # At least 1, max 20% or num_speakers\n",
        "        random_state=42,\n",
        "        stratify=None # Disable stratification as it's problematic with few samples per class\n",
        "    )\n",
        "    print(\"Proceeding with NON-STRATIFIED split due to limited samples per speaker.\")\n",
        "else:\n",
        "    train_df, test_df = train_test_split(\n",
        "        existing_files_df,\n",
        "        test_size=num_speakers, # Set test_size to the absolute number of speakers for stratification\n",
        "        random_state=42,\n",
        "        stratify=existing_files_df['speaker_id'] # This now ensures each speaker is in the test set\n",
        "    )\n",
        "    print(f\"Using STRATIFIED split with {len(test_df)} samples in test set.\")\n",
        "\n",
        "print(f\"\\n--- Dataset Split for Training and Testing ---\")\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Testing samples: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "iUqwFAEW-OIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create a Custom PyTorch Dataset with MFCCs (FINAL CORRECTED VERSION)\n",
        "# ==============================================================================\n",
        "\n",
        "class SpeakerDatasetMFCC(Dataset):\n",
        "    def __init__(self, dataframe, target_sr=16000, num_mfcc=40, n_fft=400, hop_length=160):\n",
        "        self.dataframe = dataframe\n",
        "        self.target_sr = target_sr\n",
        "        self.num_mfcc = num_mfcc\n",
        "        self.max_len_sec = 30 # Fixed duration for training\n",
        "        self.max_len_samples = self.max_len_sec * self.target_sr\n",
        "\n",
        "        self.mfcc_transform = MFCC(\n",
        "            sample_rate=target_sr, n_mfcc=num_mfcc, melkwargs={'n_fft': n_fft, 'hop_length': hop_length, 'n_mels': 128}\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        audio_path = row['audio_path']\n",
        "        label = row['speaker_id']\n",
        "\n",
        "        try:\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "            if sample_rate != self.target_sr:\n",
        "                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sr)\n",
        "                waveform = resampler(waveform)\n",
        "\n",
        "            if waveform.shape[0] > 1: # Convert to mono if stereo\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "            # Ensure waveform is 2D (channels, samples) expected by MFCC transform\n",
        "            elif waveform.ndim == 1:\n",
        "                waveform = waveform.unsqueeze(0) # Add channel dimension if it's just (samples,)\n",
        "\n",
        "            # Pad or truncate to a fixed length (max_len_samples)\n",
        "            if waveform.shape[1] > self.max_len_samples:\n",
        "                waveform = waveform[:, :self.max_len_samples]\n",
        "            elif waveform.shape[1] < self.max_len_samples:\n",
        "                padding = self.max_len_samples - waveform.shape[1]\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "\n",
        "            mfcc_features = self.mfcc_transform(waveform)\n",
        "\n",
        "            # --- CRITICAL FIX: Squeeze the channel dimension (dim=0) here ---\n",
        "            # mfcc_features original shape is (1, num_mfcc, num_frames) for mono audio\n",
        "            # We want (num_mfcc, num_frames) for the Conv1d input after batching\n",
        "            if mfcc_features.ndim == 3 and mfcc_features.shape[0] == 1:\n",
        "                mfcc_features = mfcc_features.squeeze(0)\n",
        "\n",
        "            return mfcc_features, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {e}. Skipping this sample.\")\n",
        "            return None, None\n",
        "\n",
        "# Custom collate_fn to handle None values (from failed audio loads) and ensure consistent stacking\n",
        "def collate_fn(batch):\n",
        "    batch = [item for item in batch if item[0] is not None]\n",
        "    if not batch:\n",
        "        return None, None\n",
        "\n",
        "    mfccs, labels = zip(*batch)\n",
        "\n",
        "    mfccs_stacked = torch.stack(mfccs)\n",
        "    labels_stacked = torch.stack(labels)\n",
        "\n",
        "    return mfccs_stacked, labels_stacked\n",
        "\n",
        "# Create datasets and data loaders\n",
        "train_dataset = SpeakerDatasetMFCC(train_df)\n",
        "test_dataset = SpeakerDatasetMFCC(test_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"\\n--- DataLoader created. Total training batches: {len(train_loader)} ---\")"
      ],
      "metadata": {
        "id": "aEtL-l1J-3Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define a Combined CNN-RNN Model from Scratch\n",
        "# ==============================================================================\n",
        "# This model uses CNN layers to extract local features from MFCCs and RNN (GRU)\n",
        "# layers to capture temporal dependencies, followed by a classification head.\n",
        "\n",
        "class SpeakerCNN_RNN(nn.Module):\n",
        "    def __init__(self, num_speakers, num_mfcc=40, hidden_dim=128, rnn_layers=2, dropout_rate=0.3):\n",
        "        super(SpeakerCNN_RNN, self).__init__()\n",
        "\n",
        "        # CNN layers for feature extraction from MFCCs\n",
        "        # Input: (batch_size, num_mfcc, sequence_length_frames)\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv1d(num_mfcc, 64, kernel_size=5, padding=2), # Output: (B, 64, L)\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2), # Output: (B, 64, L/2)\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=5, padding=2), # Output: (B, 128, L/2)\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2), # Output: (B, 128, L/4)\n",
        "\n",
        "            nn.Conv1d(128, 256, kernel_size=5, padding=2), # Output: (B, 256, L/4)\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2) # Output: (B, 256, L/8)\n",
        "        )\n",
        "\n",
        "        # The output of CNN layers will be (batch_size, features, new_sequence_length)\n",
        "        # We need to calculate the actual sequence length after pooling to correctly initialize RNN\n",
        "        # Let's assume input sequence length for MFCCs of 30s @ 16kHz, hop_length=160\n",
        "        # Frame length = (16000 * 30) = 480000 samples\n",
        "        # Number of frames = (480000 - 400) / 160 + 1 = ~3000 frames\n",
        "        # After 3 MaxPool1d(kernel_size=2), sequence length becomes 3000 / 2 / 2 / 2 = 375 frames\n",
        "\n",
        "        rnn_input_size = 256 # Number of features from CNN output\n",
        "\n",
        "        # RNN (GRU) layers for temporal modeling\n",
        "        # Input to RNN: (batch_size, sequence_length_frames, features)\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=rnn_input_size,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=rnn_layers,\n",
        "            bidirectional=True, # Bidirectional GRU for better context over time\n",
        "            batch_first=True # Input and output tensors are provided as (batch, seq, feature)\n",
        "        )\n",
        "\n",
        "        # Global Average Pooling after RNN to get a fixed-size embedding for classification\n",
        "        # We take the mean across the sequence dimension (dim=1)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Final fully connected layer for classification\n",
        "        # hidden_dim * 2 because of bidirectional GRU\n",
        "        self.fc_layer = nn.Linear(hidden_dim * 2, num_speakers)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, num_mfcc, sequence_length_frames)\n",
        "\n",
        "        # CNN layers\n",
        "        x = self.conv_layers(x) # Output shape: (batch_size, 256, reduced_sequence_length_frames)\n",
        "\n",
        "        # Permute for RNN input: (batch_size, sequence_length_frames, features)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # RNN layers\n",
        "        rnn_out, _ = self.rnn(x) # rnn_out shape: (batch_size, sequence_length_frames, hidden_dim * 2)\n",
        "\n",
        "        # Apply Global Average Pooling across the sequence length dimension (dim=1)\n",
        "        # Squeeze the resulting 1-dimensional output\n",
        "        # Input for global_pool needs to be (batch_size, features, sequence_length)\n",
        "        # So we permute rnn_out back\n",
        "        pooled_output = self.global_pool(rnn_out.permute(0, 2, 1)).squeeze(-1) # Output: (batch_size, hidden_dim * 2)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        x = self.dropout(pooled_output)\n",
        "\n",
        "        # Final fully connected layer for classification\n",
        "        x = self.fc_layer(x)\n",
        "        return x\n",
        "\n",
        "# Determine the device (GPU if available, else CPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\n--- Initializing model on device: {device} ---\")\n",
        "\n",
        "# Initialize the model with the correct number of speakers and MFCC features\n",
        "model = SpeakerCNN_RNN(num_speakers=num_speakers, num_mfcc=train_dataset.num_mfcc).to(device)\n",
        "\n",
        "# Define Loss function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4) # Start with a small learning rate\n"
      ],
      "metadata": {
        "id": "xgf8gUiM_r1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train the Model\n",
        "# ==============================================================================\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=100):\n",
        "    model.train()\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct_train_predictions = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\")\n",
        "        for i, (inputs, labels) in enumerate(pbar):\n",
        "            if inputs is None: # Skip batches with no valid samples\n",
        "                pbar.set_postfix_str(\"Skipping empty batch\")\n",
        "                continue\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad() # Zero the gradients\n",
        "            outputs = model(inputs) # Forward pass\n",
        "            loss = criterion(outputs, labels) # Calculate loss\n",
        "            loss.backward() # Backward pass\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train_samples += labels.size(0)\n",
        "            correct_train_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({'loss': running_loss / (i+1), 'train_acc': 100 * correct_train_predictions / total_train_samples})\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader)\n",
        "        epoch_train_accuracy = 100 * correct_train_predictions / total_train_samples\n",
        "\n",
        "        # Evaluate on the test set after each epoch\n",
        "        model.eval() # Set model to evaluation mode\n",
        "        correct_test_predictions = 0\n",
        "        total_test_samples = 0\n",
        "        test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient calculations for evaluation\n",
        "            test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Test)\")\n",
        "            for inputs, labels in test_pbar:\n",
        "                if inputs is None: continue\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_test_samples += labels.size(0)\n",
        "                correct_test_predictions += (predicted == labels).sum().item()\n",
        "                test_pbar.set_postfix({'test_loss': test_loss / (test_pbar.n + 1), 'test_acc': 100 * correct_test_predictions / total_test_samples})\n",
        "\n",
        "        epoch_test_loss = test_loss / len(test_loader)\n",
        "        epoch_test_accuracy = 100 * correct_test_predictions / total_test_samples\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Summary: Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}%, \"\n",
        "              f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_accuracy:.2f}%\")\n",
        "\n",
        "        # Save the model if it's the best one so far (based on test accuracy)\n",
        "        if epoch_test_accuracy > best_accuracy:\n",
        "            best_accuracy = epoch_test_accuracy\n",
        "            SAVE_DIR = os.path.join(PROJECT_ROOT_DIR, 'saved_models_scratch_mfcc_rnn')\n",
        "            os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "            model_save_path = os.path.join(SAVE_DIR, 'speaker_cnn_rnn_best.pth')\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"New best model saved with Test Accuracy: {best_accuracy:.2f}%\")\n",
        "\n",
        "        model.train() # Set model back to training mode for the next epoch\n",
        "\n",
        "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=100) # Training for 100 epochs\n"
      ],
      "metadata": {
        "id": "NP6IJcBH_2VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Save the Trained Model and Speaker Mapping (Final Save)\n",
        "# ==============================================================================\n",
        "# Even if a \"best\" model was saved, we save the final state as well.\n",
        "\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT_DIR, 'saved_models_scratch_mfcc_rnn')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True) # Ensure directory exists\n",
        "model_final_save_path = os.path.join(SAVE_DIR, 'speaker_cnn_rnn_final.pth')\n",
        "mapping_save_path = os.path.join(SAVE_DIR, 'speaker_mapping.joblib')\n",
        "\n",
        "joblib.dump(speaker_mapping, mapping_save_path)\n",
        "torch.save(model.state_dict(), model_final_save_path)\n",
        "\n",
        "print(f\"\\n--- Final Trained Model Saved to: {model_final_save_path} ---\")\n",
        "print(f\"Speaker Mapping Saved to: {mapping_save_path}\")\n"
      ],
      "metadata": {
        "id": "RY2pxoohZKEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 7: Inference (Detect a Person from a New Audio File - Interactive Upload)\n",
        "# --- FIX: 'SpeakerCNN_RNN' object has no attribute 'device' ---\n",
        "# ==============================================================================\n",
        "\n",
        "from google.colab import files\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import os # Ensure os is imported for path operations\n",
        "\n",
        "# --- The predict_speaker_from_audio function with the fix ---\n",
        "def predict_speaker_from_audio(model, audio_file_path, speaker_mapping,\n",
        "                               target_sr=16000, num_mfcc=40, n_fft=400, hop_length=160):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    # Ensure mfcc_transform is initialized within the function or passed as an argument\n",
        "    mfcc_transform = MFCC(sample_rate=target_sr, n_mfcc=num_mfcc, melkwargs={'n_fft': n_fft, 'hop_length': hop_length})\n",
        "    max_len_samples = 30 * target_sr # Ensure consistent audio length for inference\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(audio_file_path):\n",
        "            raise FileNotFoundError(f\"Audio file not found: {audio_file_path}\")\n",
        "\n",
        "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "        if sample_rate != target_sr:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
        "            waveform = resampler(waveform)\n",
        "        if waveform.shape[0] > 1: # Convert to mono\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "        elif waveform.ndim == 1: # Add channel dimension if it was (samples,)\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        # Pad/truncate to max_len_samples\n",
        "        if waveform.shape[1] > max_len_samples:\n",
        "            waveform = waveform[:, :max_len_samples]\n",
        "        elif waveform.shape[1] < max_len_samples:\n",
        "            padding = max_len_samples - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "\n",
        "        # Convert to MFCCs\n",
        "        mfcc_features = mfcc_transform(waveform)\n",
        "\n",
        "        # CRITICAL FIX: Squeeze the channel dimension (dim=0) for inference\n",
        "        if mfcc_features.ndim == 3 and mfcc_features.shape[0] == 1:\n",
        "            mfcc_features = mfcc_features.squeeze(0)\n",
        "\n",
        "        # --- FIX APPLIED HERE: Get device from model parameters ---\n",
        "        input_tensor = mfcc_features.unsqueeze(0).to(next(model.parameters()).device)\n",
        "\n",
        "        with torch.no_grad(): # No need to calculate gradients for inference\n",
        "            outputs = model(input_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            confidence, predicted_id_tensor = torch.max(probabilities, 1)\n",
        "\n",
        "            predicted_id = predicted_id_tensor.item()\n",
        "            predicted_confidence = confidence.item()\n",
        "\n",
        "        predicted_email = speaker_mapping[predicted_id]\n",
        "\n",
        "        return predicted_email, predicted_confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during inference for {audio_file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# --- Reload the trained model and speaker mapping for inference ---\n",
        "# These variables should already be defined from previous cells (Steps 1-6)\n",
        "# If you run this cell independently, ensure these are initialized:\n",
        "# PROJECT_ROOT_DIR = '/content/drive/MyDrive/project'\n",
        "# num_speakers = # Get this from your speaker_mapping or previous output\n",
        "# num_mfcc_features = 40 # Needs to match what you trained with (from SpeakerDatasetMFCC init)\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# loaded_model = SpeakerCNN_RNN(num_speakers=num_speakers, num_mfcc=num_mfcc_features).to(device)\n",
        "# best_model_path = os.path.join(PROJECT_ROOT_DIR, 'saved_models_scratch_mfcc_rnn', 'speaker_cnn_rnn_best.pth')\n",
        "# model_final_save_path = os.path.join(PROJECT_ROOT_DIR, 'saved_models_scratch_mfcc_rnn', 'speaker_cnn_rnn_final.pth')\n",
        "\n",
        "# if os.path.exists(best_model_path):\n",
        "#     loaded_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "#     print(f\"Loaded best model from: {best_model_path}\")\n",
        "# else:\n",
        "#     loaded_model.load_state_dict(torch.load(model_final_save_path, map_location=device))\n",
        "#     print(f\"Loaded final model from: {model_final_save_path} (Best model not found)\")\n",
        "\n",
        "# loaded_speaker_mapping = joblib.load(os.path.join(PROJECT_ROOT_DIR, 'saved_models_scratch_mfcc_rnn', 'speaker_mapping.joblib'))\n",
        "\n",
        "# Ensure existing_files_df is accessible for getting the name\n",
        "# If running this cell independently, you'd need to load it:\n",
        "# existing_files_df = pd.read_csv(os.path.join(PROJECT_ROOT_DIR, 'main_data.csv'))\n",
        "# existing_files_df['email'] = existing_files_df['email'].astype('category') # Ensure type matches the one used during training\n",
        "\n",
        "\n",
        "# --- Interactive File Upload for Prediction ---\n",
        "print(\"\\n--- Upload an audio file from your PC for speaker detection ---\")\n",
        "uploaded_files = files.upload() # This will open a file dialog\n",
        "\n",
        "if uploaded_files:\n",
        "    uploaded_file_name = list(uploaded_files.keys())[0]\n",
        "    uploaded_file_path = os.path.join('/content/', uploaded_file_name)\n",
        "\n",
        "    print(f\"\\nUploaded file: {uploaded_file_name}\")\n",
        "    print(f\"File saved to: {uploaded_file_path}\")\n",
        "\n",
        "    print(f\"\\n--- Performing Inference on the uploaded audio file ---\")\n",
        "\n",
        "    # Predict the speaker using the uploaded file\n",
        "    # Pass the num_mfcc from the trained dataset/model configuration\n",
        "    # Assuming num_mfcc was 40 during training, and train_dataset is accessible or its num_mfcc known\n",
        "    detected_email, confidence = predict_speaker_from_audio(\n",
        "        loaded_model, uploaded_file_path, loaded_speaker_mapping,\n",
        "        num_mfcc=train_dataset.num_mfcc # Use the same num_mfcc as used for training\n",
        "    )\n",
        "\n",
        "    if detected_email:\n",
        "        # Find the corresponding name from the original DataFrame\n",
        "        detected_person_df = existing_files_df[existing_files_df['email'] == detected_email]\n",
        "        detected_name = detected_person_df['name'].iloc[0] if not detected_person_df.empty else \"Unknown\"\n",
        "\n",
        "        print(\"\\n--- Detection Result ---\")\n",
        "        print(f\"The detected person is: {detected_name}\")\n",
        "        print(f\"Corresponding Email ID: {detected_email}\")\n",
        "        print(f\"Confidence: {confidence:.4f}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Detection failed for the uploaded file. Please check the audio file and its format.\")\n",
        "        print(\"Ensure it's a clear recording of one of the trained speakers.\")\n",
        "\n",
        "else:\n",
        "    print(\"No file was uploaded.\")"
      ],
      "metadata": {
        "id": "6NELW0bmZVhF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}