{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sazim2019331087/voice_model/blob/main/RNN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcIEpcLPr9Ol"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# --- GOOGLE COLAB SCRIPT FOR SPEAKER IDENTIFICATION (PERSON DETECTION) ---\n",
        "# --- (Training an RNN Only Model from Scratch using MFCC Features) ---\n",
        "# ==============================================================================\n",
        "\n",
        "# --- IMPORTANT WARNING ---\n",
        "print(\"=\"*80)\n",
        "print(\"WARNING: Training a deep learning model from scratch (RNN Only)\")\n",
        "print(\"         with only 147 audio files is highly challenging and prone to overfitting.\")\n",
        "print(\"         The model will likely have limited generalization to new voices not in your dataset.\")\n",
        "print(\"         This code is provided for educational purposes to demonstrate the architecture.\")\n",
        "print(\"         Ensure you have a GPU runtime enabled in Colab, as CPU training will be extremely slow.\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qng7W-QHs7kw"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Step 1: Setup and Install Libraries\n",
        "# ==============================================================================\n",
        "print(\"--- Installing required libraries ---\")\n",
        "!pip install --upgrade pip\n",
        "# CRITICAL: Re-install PyTorch and TorchAudio to ensure CUDA version compatibility\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install pandas scikit-learn joblib tqdm\n",
        "!pip install ffmpeg-python\n",
        "!apt-get update && !apt-get install -y ffmpeg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgQcYQWqtzEJ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Step 2: Mount Google Drive and Load Data\n",
        "# ==============================================================================\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchaudio\n",
        "from torchaudio.transforms import MFCC\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "\n",
        "print(\"\\n--- Mounting Google Drive ---\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT_DIR = '/content/drive/MyDrive/project'\n",
        "CSV_PATH = os.path.join(PROJECT_ROOT_DIR, 'training.csv')\n",
        "AUDIO_FOLDER_PATH = os.path.join(PROJECT_ROOT_DIR, 'voices')\n",
        "\n",
        "if not os.path.exists(PROJECT_ROOT_DIR):\n",
        "    raise FileNotFoundError(f\"Error: Project folder '{PROJECT_ROOT_DIR}' not found. Please check the path and your Google Drive structure.\")\n",
        "elif not os.path.exists(CSV_PATH):\n",
        "    raise FileNotFoundError(f\"Error: CSV file '{CSV_PATH}' not found. Please ensure it's in the correct location.\")\n",
        "elif not os.path.exists(AUDIO_FOLDER_PATH):\n",
        "    raise FileNotFoundError(f\"Error: Audio folder '{AUDIO_FOLDER_PATH}' not found. Please check the path and upload your audio files.\")\n",
        "else:\n",
        "    print(f\"Successfully located project folder at: {PROJECT_ROOT_DIR}\")\n",
        "\n",
        "print(\"\\n--- Loading data from CSV ---\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df['audio_path'] = df['audio_file'].apply(lambda x: os.path.join(AUDIO_FOLDER_PATH, x))\n",
        "\n",
        "print(\"\\n--- Verifying audio file paths and formats... ---\")\n",
        "verified_data_for_df = []\n",
        "\n",
        "test_mfcc_transform = MFCC(\n",
        "    sample_rate=16000, n_mfcc=40, melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 128}\n",
        ")\n",
        "\n",
        "problematic_files = []\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Verifying audio files\"):\n",
        "    audio_file_path = row['audio_path']\n",
        "\n",
        "    if not os.path.exists(audio_file_path):\n",
        "        problematic_files.append((row['audio_file'], row['email'], \"File Not Found\"))\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        waveform, sample_rate = torchaudio.load(audio_file_path, frame_offset=0, num_frames=16000 * 2)\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            waveform = resampler(waveform)\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "        elif waveform.ndim == 1:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        temp_mfcc_features = test_mfcc_transform(waveform)\n",
        "\n",
        "        if temp_mfcc_features.ndim != 3 or temp_mfcc_features.shape[0] != 1:\n",
        "            raise ValueError(f\"Initial MFCC features for {audio_file_path} unexpected shape: {temp_mfcc_features.shape}\")\n",
        "\n",
        "        verified_data_for_df.append(row.to_dict())\n",
        "\n",
        "    except Exception as e:\n",
        "        problematic_files.append((row['audio_file'], speaker_id_raw, f\"Format Error: {e}\"))\n",
        "\n",
        "if problematic_files:\n",
        "    print(f\"\\n--- {len(problematic_files)} Problematic audio files found and skipped ---\")\n",
        "    problematic_df = pd.DataFrame(problematic_files, columns=['audio_file', 'email', 'Reason'])\n",
        "    print(problematic_df.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "    print(\"\\nTip: Use `ffmpeg -i input.wav -ar 16000 -ac 1 -c:a pcm_s16le output_converted.wav` to convert problematic files.\")\n",
        "else:\n",
        "    print(\"\\nAll audio files verified successfully!\")\n",
        "\n",
        "if not verified_data_for_df:\n",
        "    raise ValueError(\"No valid audio files found after verification. Please check your data.\")\n",
        "\n",
        "existing_files_df = pd.DataFrame(verified_data_for_df)\n",
        "existing_files_df['speaker_id'] = existing_files_df['email'].astype('category').cat.codes\n",
        "speaker_mapping = dict(enumerate(existing_files_df['email'].astype('category').cat.categories))\n",
        "num_speakers = len(speaker_mapping)\n",
        "\n",
        "print(f\"\\n--- Speaker Mapping (Total Unique Speakers: {num_speakers}) ---\")\n",
        "print(speaker_mapping)\n",
        "\n",
        "if len(existing_files_df) < num_speakers * 2:\n",
        "    print(\"\\nWARNING: Dataset has very few samples per speaker. Stratification might be difficult.\")\n",
        "    print(f\"Total samples: {len(existing_files_df)}, Unique speakers: {num_speakers}\")\n",
        "    train_df, test_df = train_test_split(\n",
        "        existing_files_df,\n",
        "        test_size=max(1, min(int(0.2 * len(existing_files_df)), num_speakers)),\n",
        "        random_state=42,\n",
        "        stratify=None\n",
        "    )\n",
        "    print(\"Proceeding with NON-STRATIFIED split due to limited samples per speaker.\")\n",
        "else:\n",
        "    train_df, test_df = train_test_split(\n",
        "        existing_files_df,\n",
        "        test_size=num_speakers,\n",
        "        random_state=42,\n",
        "        stratify=existing_files_df['speaker_id']\n",
        "    )\n",
        "    print(f\"Using STRATIFIED split with {len(test_df)} samples in test set.\")\n",
        "\n",
        "print(f\"\\n--- Dataset Split for Training and Testing ---\\nTraining samples: {len(train_df)}\\nTesting samples: {len(test_df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlvIFRIAu5ob"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Step 3: Create a Custom PyTorch Dataset with MFCCs\n",
        "# ==============================================================================\n",
        "class SpeakerDatasetMFCC(Dataset):\n",
        "    def __init__(self, dataframe, target_sr=16000, num_mfcc=40, n_fft=400, hop_length=160):\n",
        "        self.dataframe = dataframe\n",
        "        self.target_sr = target_sr\n",
        "        self.num_mfcc = num_mfcc\n",
        "        self.max_len_sec = 30\n",
        "        self.max_len_samples = self.max_len_sec * self.target_sr\n",
        "\n",
        "        self.mfcc_transform = MFCC(\n",
        "            sample_rate=target_sr, n_mfcc=num_mfcc, melkwargs={'n_fft': n_fft, 'hop_length': hop_length, 'n_mels': 128}\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        audio_path = row['audio_path']\n",
        "        label = row['speaker_id']\n",
        "\n",
        "        try:\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "            if sample_rate != self.target_sr:\n",
        "                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sr)\n",
        "                waveform = resampler(waveform)\n",
        "\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "            elif waveform.ndim == 1:\n",
        "                waveform = waveform.unsqueeze(0)\n",
        "\n",
        "            if waveform.shape[1] > self.max_len_samples:\n",
        "                waveform = waveform[:, :self.max_len_samples]\n",
        "            elif waveform.shape[1] < self.max_len_samples:\n",
        "                padding = self.max_len_samples - waveform.shape[1]\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "\n",
        "            mfcc_features = self.mfcc_transform(waveform)\n",
        "\n",
        "            if mfcc_features.ndim == 3 and mfcc_features.shape[0] == 1:\n",
        "                mfcc_features = mfcc_features.squeeze(0)\n",
        "\n",
        "            return mfcc_features, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {e}. Skipping this sample.\")\n",
        "            return None, None\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = [item for item in batch if item[0] is not None]\n",
        "    if not batch:\n",
        "        return None, None\n",
        "\n",
        "    mfccs, labels = zip(*batch)\n",
        "\n",
        "    mfccs_stacked = torch.stack(mfccs)\n",
        "    labels_stacked = torch.stack(labels)\n",
        "\n",
        "    return mfccs_stacked, labels_stacked\n",
        "\n",
        "train_dataset = SpeakerDatasetMFCC(train_df)\n",
        "test_dataset = SpeakerDatasetMFCC(test_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"\\n--- DataLoader created. Total training batches: {len(train_loader)} ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssi7vWZeu92e"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Step 4: Define an RNN Model (SpeakerRNN)\n",
        "# ==============================================================================\n",
        "\n",
        "class SpeakerRNN(nn.Module):\n",
        "    def __init__(self, num_speakers, num_mfcc=40, hidden_dim=128, rnn_layers=2, dropout_rate=0.3):\n",
        "        super(SpeakerRNN, self).__init__()\n",
        "\n",
        "        print(\"\\n--- Initializing SpeakerRNN Model Architecture (ONLY RNN) ---\")\n",
        "        print(f\"Number of speakers (output classes): {num_speakers}\")\n",
        "        print(f\"Number of MFCC features (input features for RNN): {num_mfcc}\")\n",
        "        print(f\"RNN hidden dimension: {hidden_dim}\")\n",
        "        print(f\"Number of RNN layers: {rnn_layers}\")\n",
        "        print(f\"Dropout rate: {dropout_rate}\")\n",
        "\n",
        "        # RNN (GRU) layers for temporal modeling\n",
        "        # Input to RNN: (batch_size, sequence_length_frames, num_mfcc)\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=num_mfcc, # RNN input size is directly the number of MFCC features\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=rnn_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "        print(f\"  RNN Layer: GRU(input={num_mfcc}, hidden={hidden_dim}, layers={rnn_layers}, bidirectional=True)\")\n",
        "\n",
        "        # Global Average Pooling after RNN to get a fixed-size embedding for classification\n",
        "        # Output of RNN is (batch_size, sequence_length, hidden_dim * 2)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        print(f\"  Global Pooling Layer: AdaptiveAvgPool1d(1) (summarizes RNN output sequence)\")\n",
        "\n",
        "        # Final fully connected layer for classification\n",
        "        self.fc_layer = nn.Linear(hidden_dim * 2, num_speakers) # hidden_dim * 2 for bidirectional\n",
        "        print(f\"  Fully Connected Layer: Linear(in={hidden_dim * 2}, out={num_speakers})\")\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        print(f\"  Dropout Layer: Dropout(p={dropout_rate})\")\n",
        "        print(\"--- Model Initialization Complete ---\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, num_mfcc, sequence_length_frames)\n",
        "        # For RNN, input needs to be (batch_size, sequence_length_frames, num_mfcc)\n",
        "\n",
        "        # Permute MFCCs for RNN input\n",
        "        x = x.permute(0, 2, 1) # Output shape: (batch_size, sequence_length_frames, num_mfcc)\n",
        "\n",
        "        # RNN layers\n",
        "        rnn_out, _ = self.rnn(x) # rnn_out shape: (batch_size, sequence_length_frames, hidden_dim * 2)\n",
        "\n",
        "        # Apply Global Average Pooling across the sequence length dimension (dim=1)\n",
        "        pooled_output = self.global_pool(rnn_out.permute(0, 2, 1)).squeeze(-1) # Output: (batch_size, hidden_dim * 2)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        x = self.dropout(pooled_output)\n",
        "\n",
        "        # Final fully connected layer for classification\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Determine the device (GPU if available, else CPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\n--- Initializing model on device: {device} ---\")\n",
        "\n",
        "# Initialize the SpeakerRNN model\n",
        "model = SpeakerRNN(num_speakers=num_speakers, num_mfcc=train_dataset.num_mfcc).to(device)\n",
        "\n",
        "# Define Loss function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeyFK25AveWZ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Step 5: Train the Model\n",
        "# ==============================================================================\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=100):\n",
        "    model.train()\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct_train_predictions = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\")\n",
        "        for i, (inputs, labels) in enumerate(pbar):\n",
        "            if inputs is None:\n",
        "                pbar.set_postfix_str(\"Skipping empty batch\")\n",
        "                continue\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train_samples += labels.size(0)\n",
        "            correct_train_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({'loss': running_loss / (i+1), 'train_acc': 100 * correct_train_predictions / total_train_samples})\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader)\n",
        "        epoch_train_accuracy = 100 * correct_train_predictions / total_train_samples\n",
        "\n",
        "        model.eval()\n",
        "        correct_test_predictions = 0\n",
        "        total_test_samples = 0\n",
        "        test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Test)\")\n",
        "            for inputs, labels in test_pbar:\n",
        "                if inputs is None: continue\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_test_samples += labels.size(0)\n",
        "                correct_test_predictions += (predicted == labels).sum().item()\n",
        "                test_pbar.set_postfix({'test_loss': test_loss / (test_pbar.n + 1), 'test_acc': 100 * correct_test_predictions / total_test_samples})\n",
        "\n",
        "        epoch_test_loss = test_loss / len(test_loader)\n",
        "        epoch_test_accuracy = 100 * correct_test_predictions / total_test_samples\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Summary: Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}%, \"\n",
        "              f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_accuracy:.2f}%\")\n",
        "\n",
        "        if epoch_test_accuracy > best_accuracy:\n",
        "            best_accuracy = epoch_test_accuracy\n",
        "            SAVE_DIR = os.path.join(PROJECT_ROOT_DIR, 'saved_models_rnn_only') # Changed save directory\n",
        "            os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "            model_save_path = os.path.join(SAVE_DIR, 'speaker_rnn_best.pth') # Changed model name\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"New best model saved with Test Accuracy: {best_accuracy:.2f}%\")\n",
        "\n",
        "        model.train()\n",
        "\n",
        "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 6: Save the Trained Model and Speaker Mapping (Final Save)\n",
        "# ==============================================================================\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT_DIR, 'saved_models_rnn_only') # Changed save directory\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "model_final_save_path = os.path.join(SAVE_DIR, 'speaker_rnn_final.pth') # Changed model name\n",
        "mapping_save_path = os.path.join(SAVE_DIR, 'speaker_mapping_rnn_only.joblib') # Changed mapping name\n",
        "\n",
        "joblib.dump(speaker_mapping, mapping_save_path)\n",
        "torch.save(model.state_dict(), model_final_save_path)\n",
        "\n",
        "print(f\"\\n--- Final Trained Model Saved to: {model_final_save_path} ---\")\n",
        "print(f\"Speaker Mapping Saved to: {mapping_save_path}\")"
      ],
      "metadata": {
        "id": "tc9yd2ZD8qMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 7: Inference (Detect a Person from a New Audio File - Interactive Upload)\n",
        "# ==============================================================================\n",
        "from google.colab import files\n",
        "import soundfile as sf\n",
        "\n",
        "def predict_speaker_from_audio(model, audio_file_path, speaker_mapping,\n",
        "                               target_sr=16000, num_mfcc=40, n_fft=400, hop_length=160):\n",
        "    model.eval()\n",
        "    mfcc_transform = MFCC(sample_rate=target_sr, n_mfcc=num_mfcc, melkwargs={'n_fft': n_fft, 'hop_length': hop_length})\n",
        "    max_len_samples = 30 * target_sr\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(audio_file_path):\n",
        "            raise FileNotFoundError(f\"Audio file not found: {audio_file_path}\")\n",
        "\n",
        "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "        if sample_rate != target_sr:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
        "            waveform = resampler(waveform)\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "        elif waveform.ndim == 1:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        if waveform.shape[1] > max_len_samples:\n",
        "            waveform = waveform[:, :max_len_samples]\n",
        "        elif waveform.shape[1] < max_len_samples:\n",
        "            padding = max_len_samples - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "\n",
        "        mfcc_features = mfcc_transform(waveform)\n",
        "\n",
        "        if mfcc_features.ndim == 3 and mfcc_features.shape[0] == 1:\n",
        "            mfcc_features = mfcc_features.squeeze(0)\n",
        "\n",
        "        input_tensor = mfcc_features.unsqueeze(0).to(next(model.parameters()).device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            confidence, predicted_id_tensor = torch.max(probabilities, 1)\n",
        "\n",
        "            predicted_id = predicted_id_tensor.item()\n",
        "            predicted_confidence = confidence.item()\n",
        "\n",
        "        predicted_email = speaker_mapping[predicted_id]\n",
        "\n",
        "        return predicted_email, predicted_confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during inference for {audio_file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Reload the best saved model and mapping for inference\n",
        "loaded_model = SpeakerRNN(num_speakers=num_speakers, num_mfcc=train_dataset.num_mfcc).to(device) # Changed model class\n",
        "best_model_path = os.path.join(PROJECT_ROOT_DIR, 'saved_models_rnn_only', 'speaker_rnn_best.pth') # Changed path/name\n",
        "model_final_save_path = os.path.join(PROJECT_ROOT_DIR, 'saved_models_rnn_only', 'speaker_rnn_final.pth') # Changed path/name\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    loaded_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    print(f\"Loaded best model from: {best_model_path}\")\n",
        "else:\n",
        "    loaded_model.load_state_dict(torch.load(model_final_save_path, map_location=device))\n",
        "    print(f\"Loaded final model from: {model_final_save_path} (Best model not found)\")\n",
        "\n",
        "loaded_speaker_mapping = joblib.load(os.path.join(PROJECT_ROOT_DIR, 'saved_models_rnn_only', 'speaker_mapping_rnn_only.joblib')) # Changed mapping name\n",
        "\n",
        "# Load existing_files_df to get names\n",
        "try:\n",
        "    full_df = pd.read_csv(CSV_PATH)\n",
        "    full_df['audio_path'] = full_df['audio_ffile'].apply(lambda x: os.path.join(AUDIO_FOLDER_PATH, x))\n",
        "    existing_files_df_temp = full_df[full_df['audio_path'].apply(os.path.exists)].copy().reset_index(drop=True)\n",
        "    existing_files_df_temp['speaker_id'] = existing_files_df_temp['email'].astype('category').cat.codes\n",
        "    existing_files_df = existing_files_df_temp\n",
        "    print(\"Reloaded existing_files_df for name lookup.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reloading existing_files_df for name lookup: {e}\")\n",
        "    existing_files_df = pd.DataFrame({'email': [], 'name': []})\n",
        "\n",
        "\n",
        "print(\"\\n--- Upload an audio file from your PC for speaker detection ---\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "if uploaded_files:\n",
        "    uploaded_file_name = list(uploaded_files.keys())[0]\n",
        "    uploaded_file_path = os.path.join('/content/', uploaded_file_name)\n",
        "\n",
        "    print(f\"\\nUploaded file: {uploaded_file_name}\")\n",
        "    print(f\"File saved to: {uploaded_file_path}\")\n",
        "\n",
        "    print(f\"\\n--- Performing Inference on the uploaded audio file ---\")\n",
        "\n",
        "    detected_email, confidence = predict_speaker_from_audio(\n",
        "        loaded_model, uploaded_file_path, loaded_speaker_mapping,\n",
        "        num_mfcc=train_dataset.num_mfcc\n",
        "    )\n",
        "\n",
        "    if detected_email:\n",
        "        print(\"\\n--- Detection Result ---\")\n",
        "        print(f\"Corresponding Email ID: {detected_email}\")\n",
        "        print(f\"Confidence: {confidence:.4f}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Detection failed for the uploaded file. No matching speaker found or an error occurred.\")\n",
        "        print(\"Ensure it's a clear recording of one of the trained speakers.\")\n",
        "        print(f\"Best confidence achieved (if any): {confidence:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"No file was uploaded.\")\n"
      ],
      "metadata": {
        "id": "ralRGBqz9BVs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM18BBu8ksPXbSMttY8IzhB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}