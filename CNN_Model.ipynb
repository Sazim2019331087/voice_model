{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpDxMwOBoS2jg3fnQtXeIT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sazim2019331087/voice_model/blob/main/CNN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR_p26NkMuI0"
      },
      "outputs": [],
      "source": [
        "# --- IMPORTANT WARNING ---\n",
        "print(\"=\"*80)\n",
        "print(\"WARNING: Training a deep learning model from scratch (CNN Only)\")\n",
        "print(\"         with only 147 audio files is highly challenging and prone to overfitting.\")\n",
        "print(\"         The model will likely have limited generalization to new voices not in your dataset.\")\n",
        "print(\"         This code is provided for educational purposes to demonstrate the architecture.\")\n",
        "print(\"         Ensure you have a GPU runtime enabled in Colab, as CPU training will be extremely slow.\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 1: Setup and Install Libraries\n",
        "# ==============================================================================\n",
        "print(\"--- Installing required libraries ---\")\n",
        "!pip install --upgrade pip\n",
        "# CRITICAL: Re-install PyTorch and TorchAudio to ensure CUDA version compatibility\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install pandas scikit-learn joblib tqdm\n",
        "!pip install ffmpeg-python\n",
        "!apt-get update && !apt-get install -y ffmpeg"
      ],
      "metadata": {
        "id": "FYv6JLo0M3Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 2: Mount Google Drive and Load Data\n",
        "# ==============================================================================\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchaudio\n",
        "from torchaudio.transforms import MFCC\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "\n",
        "print(\"\\n--- Mounting Google Drive ---\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT_DIR = '/content/drive/MyDrive/project'\n",
        "CSV_PATH = os.path.join(PROJECT_ROOT_DIR, 'training.csv')\n",
        "AUDIO_FOLDER_PATH = os.path.join(PROJECT_ROOT_DIR, 'voices')\n",
        "\n",
        "if not os.path.exists(PROJECT_ROOT_DIR):\n",
        "    raise FileNotFoundError(f\"Error: Project folder '{PROJECT_ROOT_DIR}' not found. Please check the path and your Google Drive structure.\")\n",
        "elif not os.path.exists(CSV_PATH):\n",
        "    raise FileNotFoundError(f\"Error: CSV file '{CSV_PATH}' not found. Please ensure it's in the correct location.\")\n",
        "elif not os.path.exists(AUDIO_FOLDER_PATH):\n",
        "    raise FileNotFoundError(f\"Error: Audio folder '{AUDIO_FOLDER_PATH}' not found. Please check the path and upload your audio files.\")\n",
        "else:\n",
        "    print(f\"Successfully located project folder at: {PROJECT_ROOT_DIR}\")\n",
        "\n",
        "print(\"\\n--- Loading data from CSV ---\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df['audio_path'] = df['audio_file'].apply(lambda x: os.path.join(AUDIO_FOLDER_PATH, x))\n",
        "\n",
        "print(\"\\n--- Verifying audio file paths and formats... ---\")\n",
        "verified_data_for_df = []\n",
        "\n",
        "test_mfcc_transform = MFCC(\n",
        "    sample_rate=16000, n_mfcc=40, melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 128}\n",
        ")\n",
        "\n",
        "problematic_files = []\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Verifying audio files\"):\n",
        "    audio_file_path = row['audio_path']\n",
        "\n",
        "    if not os.path.exists(audio_file_path):\n",
        "        problematic_files.append((row['audio_file'], row['email'], \"File Not Found\"))\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        waveform, sample_rate = torchaudio.load(audio_file_path, frame_offset=0, num_frames=16000 * 2)\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            waveform = resampler(waveform)\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "        elif waveform.ndim == 1:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        temp_mfcc_features = test_mfcc_transform(waveform)\n",
        "\n",
        "        if temp_mfcc_features.ndim != 3 or temp_mfcc_features.shape[0] != 1:\n",
        "            raise ValueError(f\"Initial MFCC features for {audio_file_path} unexpected shape: {temp_mfcc_features.shape}\")\n",
        "\n",
        "        verified_data_for_df.append(row.to_dict())\n",
        "\n",
        "    except Exception as e:\n",
        "        problematic_files.append((row['audio_file'], row['email'], f\"Format Error: {e}\"))\n",
        "\n",
        "if problematic_files:\n",
        "    print(f\"\\n--- {len(problematic_files)} Problematic audio files found and skipped ---\")\n",
        "    problematic_df = pd.DataFrame(problematic_files, columns=['audio_file', 'email', 'Reason'])\n",
        "    print(problematic_df.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "    print(\"\\nTip: Use `ffmpeg -i input.wav -ar 16000 -ac 1 -c:a pcm_s16le output_converted.wav` to convert problematic files.\")\n",
        "else:\n",
        "    print(\"\\nAll audio files verified successfully!\")\n",
        "\n",
        "if not verified_data_for_df:\n",
        "    raise ValueError(\"No valid audio files found after verification. Please check your data.\")\n",
        "\n",
        "existing_files_df = pd.DataFrame(verified_data_for_df)\n",
        "existing_files_df['speaker_id'] = existing_files_df['email'].astype('category').cat.codes\n",
        "speaker_mapping = dict(enumerate(existing_files_df['email'].astype('category').cat.categories))\n",
        "num_speakers = len(speaker_mapping)\n",
        "\n",
        "print(f\"\\n--- Speaker Mapping (Total Unique Speakers: {num_speakers}) ---\")\n",
        "print(speaker_mapping)\n",
        "\n",
        "if len(existing_files_df) < num_speakers * 2:\n",
        "    print(\"\\nWARNING: Dataset has very few samples per speaker. Stratification might be difficult.\")\n",
        "    print(f\"Total samples: {len(existing_files_df)}, Unique speakers: {num_speakers}\")\n",
        "    train_df, test_df = train_test_split(\n",
        "        existing_files_df,\n",
        "        test_size=max(1, min(int(0.2 * len(existing_files_df)), num_speakers)),\n",
        "        random_state=42,\n",
        "        stratify=None\n",
        "    )\n",
        "    print(\"Proceeding with NON-STRATIFIED split due to limited samples per speaker.\")\n",
        "else:\n",
        "    train_df, test_df = train_test_split(\n",
        "        existing_files_df,\n",
        "        test_size=num_speakers,\n",
        "        random_state=42,\n",
        "        stratify=existing_files_df['speaker_id']\n",
        "    )\n",
        "    print(f\"Using STRATIFIED split with {len(test_df)} samples in test set.\")\n",
        "\n",
        "print(f\"\\n--- Dataset Split for Training and Testing ---\\nTraining samples: {len(train_df)}\\nTesting samples: {len(test_df)}\")\n"
      ],
      "metadata": {
        "id": "HfybFlI3OIWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 3: Create a Custom PyTorch Dataset with MFCCs\n",
        "# ==============================================================================\n",
        "class SpeakerDatasetMFCC(Dataset):\n",
        "    def __init__(self, dataframe, target_sr=16000, num_mfcc=40, n_fft=400, hop_length=160):\n",
        "        self.dataframe = dataframe\n",
        "        self.target_sr = target_sr\n",
        "        self.num_mfcc = num_mfcc\n",
        "        self.max_len_sec = 30\n",
        "        self.max_len_samples = self.max_len_sec * self.target_sr\n",
        "\n",
        "        self.mfcc_transform = MFCC(\n",
        "            sample_rate=target_sr, n_mfcc=num_mfcc, melkwargs={'n_fft': n_fft, 'hop_length': hop_length, 'n_mels': 128}\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        audio_path = row['audio_path']\n",
        "        label = row['speaker_id']\n",
        "\n",
        "        try:\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "            if sample_rate != self.target_sr:\n",
        "                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sr)\n",
        "                waveform = resampler(waveform)\n",
        "\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "            elif waveform.ndim == 1:\n",
        "                waveform = waveform.unsqueeze(0)\n",
        "\n",
        "            if waveform.shape[1] > self.max_len_samples:\n",
        "                waveform = waveform[:, :self.max_len_samples]\n",
        "            elif waveform.shape[1] < self.max_len_samples:\n",
        "                padding = self.max_len_samples - waveform.shape[1]\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "\n",
        "            mfcc_features = self.mfcc_transform(waveform)\n",
        "\n",
        "            if mfcc_features.ndim == 3 and mfcc_features.shape[0] == 1:\n",
        "                mfcc_features = mfcc_features.squeeze(0)\n",
        "\n",
        "            return mfcc_features, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {e}. Skipping this sample.\")\n",
        "            return None, None\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = [item for item in batch if item[0] is not None]\n",
        "    if not batch:\n",
        "        return None, None\n",
        "\n",
        "    mfccs, labels = zip(*batch)\n",
        "\n",
        "    mfccs_stacked = torch.stack(mfccs)\n",
        "    labels_stacked = torch.stack(labels)\n",
        "\n",
        "    return mfccs_stacked, labels_stacked\n",
        "\n",
        "train_dataset = SpeakerDatasetMFCC(train_df)\n",
        "test_dataset = SpeakerDatasetMFCC(test_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"\\n--- DataLoader created. Total training batches: {len(train_loader)} ---\")\n"
      ],
      "metadata": {
        "id": "tpG415g1O7zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 4: Define a CNN Only Model (SpeakerCNN)\n",
        "# ==============================================================================\n",
        "\n",
        "class SpeakerCNN(nn.Module):\n",
        "    def __init__(self, num_speakers, num_mfcc=40, dropout_rate=0.3):\n",
        "        super(SpeakerCNN, self).__init__()\n",
        "\n",
        "        print(\"\\n--- Initializing SpeakerCNN Model Architecture (ONLY CNN) ---\")\n",
        "        print(f\"Number of speakers (output classes): {num_speakers}\")\n",
        "        print(f\"Number of MFCC features (input channels for CNN): {num_mfcc}\")\n",
        "        print(f\"Dropout rate: {dropout_rate}\")\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv1d(num_mfcc, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "\n",
        "            nn.Conv1d(128, 256, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2)\n",
        "        )\n",
        "        print(\"  CNN Layers defined.\")\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        print(f\"  Global Pooling Layer: AdaptiveAvgPool1d(1) (summarizes CNN output sequence)\")\n",
        "\n",
        "        self.fc_layer = nn.Linear(256, num_speakers)\n",
        "        print(f\"  Fully Connected Layer: Linear(in=256, out={num_speakers})\")\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        print(f\"  Dropout Layer: Dropout(p={dropout_rate})\")\n",
        "        print(\"--- Model Initialization Complete ---\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f\"\\nDEBUG: Forward Pass (SpeakerCNN) - Input shape to model: {x.shape}\")\n",
        "\n",
        "        x = self.conv_layers(x)\n",
        "        print(f\"DEBUG: Forward Pass (SpeakerCNN) - Shape after CNN layers: {x.shape}\")\n",
        "\n",
        "        pooled_output = self.global_pool(x).squeeze(-1)\n",
        "        print(f\"DEBUG: Forward Pass (SpeakerCNN) - Shape after Global Pooling: {pooled_output.shape}\")\n",
        "\n",
        "        x = self.dropout(pooled_output)\n",
        "        print(f\"DEBUG: Forward Pass (SpeakerCNN) - Shape after Dropout: {x.shape}\")\n",
        "\n",
        "        x = self.fc_layer(x)\n",
        "        print(f\"DEBUG: Forward Pass (SpeakerCNN) - Final output shape (logits): {x.shape}\")\n",
        "        return x\n",
        "\n",
        "# Determine the device (GPU if available, else CPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\n--- Initializing model on device: {device} ---\")\n",
        "\n",
        "# Initialize the SpeakerCNN model\n",
        "model = SpeakerCNN(num_speakers=num_speakers, num_mfcc=train_dataset.num_mfcc).to(device)\n",
        "\n",
        "# Define Loss function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "Q_kJUB9TPHlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 5: Train the Model\n",
        "# ==============================================================================\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=100):\n",
        "    model.train()\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct_train_predictions = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        # tqdm progress bar for training batches\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\")\n",
        "        for i, (inputs, labels) in enumerate(pbar):\n",
        "            if inputs is None: # Skip batches with no valid samples\n",
        "                pbar.set_postfix_str(\"Skipping empty batch\")\n",
        "                continue\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad() # Zero the gradients\n",
        "            outputs = model(inputs) # Forward pass\n",
        "            loss = criterion(outputs, labels) # Calculate loss\n",
        "            loss.backward() # Backward pass\n",
        "            optimizer.step() # Update weights\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train_samples += labels.size(0)\n",
        "            correct_train_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "            # Update progress bar postfix with current batch loss and accuracy\n",
        "            pbar.set_postfix({'loss': running_loss / (i+1), 'train_acc': 100 * correct_train_predictions / total_train_samples})\n",
        "\n",
        "        # Calculate epoch-level training loss and accuracy\n",
        "        epoch_train_loss = running_loss / len(train_loader)\n",
        "        epoch_train_accuracy = 100 * correct_train_predictions / total_train_samples\n",
        "\n",
        "        # Evaluate on the test set after each epoch\n",
        "        model.eval() # Set model to evaluation mode (disables dropout, etc.)\n",
        "        correct_test_predictions = 0\n",
        "        total_test_samples = 0\n",
        "        test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient calculations for evaluation\n",
        "            # tqdm progress bar for test batches\n",
        "            test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Test)\")\n",
        "            for inputs, labels in test_pbar:\n",
        "                if inputs is None: continue # Skip empty batches\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_test_samples += labels.size(0)\n",
        "                correct_test_predictions += (predicted == labels).sum().item()\n",
        "                # Update progress bar postfix with current test loss and accuracy\n",
        "                test_pbar.set_postfix({'test_loss': test_loss / (test_pbar.n + 1), 'test_acc': 100 * correct_test_predictions / total_test_samples})\n",
        "\n",
        "        # Calculate epoch-level test loss and accuracy\n",
        "        epoch_test_loss = test_loss / len(test_loader)\n",
        "        epoch_test_accuracy = 100 * correct_test_predictions / total_test_samples\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch+1} Summary: Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}%, \"\n",
        "              f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_accuracy:.2f}%\")\n",
        "\n",
        "        # Save the model if it's the best one so far (based on test accuracy)\n",
        "        # Ensure SAVE_DIR and model_save_path are correctly defined for the specific model (CNN/RNN)\n",
        "        if epoch_test_accuracy > best_accuracy:\n",
        "            best_accuracy = epoch_test_accuracy\n",
        "            # These paths should be defined globally in your script's Step 6 section\n",
        "            # For CNN Only:\n",
        "            SAVE_DIR = os.path.join(PROJECT_ROOT_DIR, 'saved_models_cnn_only')\n",
        "            model_save_path = os.path.join(SAVE_DIR, 'speaker_cnn_best.pth')\n",
        "            # For RNN Only:\n",
        "            # SAVE_DIR = os.path.join(PROJECT_ROOT_DIR, 'saved_models_rnn_only')\n",
        "            # model_save_path = os.path.join(SAVE_DIR, 'speaker_rnn_best.pth')\n",
        "\n",
        "            os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"New best model saved with Test Accuracy: {best_accuracy:.2f}%\")\n",
        "\n",
        "        model.train() # Set model back to training mode for the next epoch\n",
        "\n",
        "# Call the train_model function to start training\n",
        "# Ensure model, train_loader, test_loader, criterion, optimizer, and num_epochs are defined.\n",
        "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=100)"
      ],
      "metadata": {
        "id": "gqlNPO5xPSIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 6: Save the Trained Model and Speaker Mapping (Final Save)\n",
        "# ==============================================================================\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT_DIR, 'saved_models_cnn_only') # Changed save directory\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "model_final_save_path = os.path.join(SAVE_DIR, 'speaker_cnn_final.pth') # Changed model name\n",
        "mapping_save_path = os.path.join(SAVE_DIR, 'speaker_mapping_cnn_only.joblib') # Changed mapping name\n",
        "\n",
        "joblib.dump(speaker_mapping, mapping_save_path)\n",
        "torch.save(model.state_dict(), model_final_save_path)\n",
        "\n",
        "print(f\"\\n--- Final Trained Model Saved to: {model_final_save_path} ---\")\n",
        "print(f\"Speaker Mapping Saved to: {mapping_save_path}\")"
      ],
      "metadata": {
        "id": "w2w05Hvqf0Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Step 7: Inference (Detect a Person from a New Audio File - Interactive Upload)\n",
        "# ==============================================================================\n",
        "from google.colab import files\n",
        "import soundfile as sf\n",
        "\n",
        "def predict_speaker_from_audio(model, audio_file_path, speaker_mapping,\n",
        "                               target_sr=16000, num_mfcc=40, n_fft=400, hop_length=160):\n",
        "    model.eval()\n",
        "    mfcc_transform = MFCC(sample_rate=target_sr, n_mfcc=num_mfcc, melkwargs={'n_fft': n_fft, 'hop_length': hop_length})\n",
        "    max_len_samples = 30 * target_sr\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(audio_file_path):\n",
        "            raise FileNotFoundError(f\"Audio file not found: {audio_file_path}\")\n",
        "\n",
        "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "        if sample_rate != target_sr:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
        "            waveform = resampler(waveform)\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "        elif waveform.ndim == 1:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        if waveform.shape[1] > max_len_samples:\n",
        "            waveform = waveform[:, :max_len_samples]\n",
        "        elif waveform.shape[1] < max_len_samples:\n",
        "            padding = max_len_samples - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "\n",
        "        mfcc_features = mfcc_transform(waveform)\n",
        "\n",
        "        if mfcc_features.ndim == 3 and mfcc_features.shape[0] == 1:\n",
        "            mfcc_features = mfcc_features.squeeze(0)\n",
        "\n",
        "        input_tensor = mfcc_features.unsqueeze(0).to(next(model.parameters()).device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            confidence, predicted_id_tensor = torch.max(probabilities, 1)\n",
        "\n",
        "            predicted_id = predicted_id_tensor.item()\n",
        "            predicted_confidence = confidence.item()\n",
        "\n",
        "        predicted_email = speaker_mapping[predicted_id]\n",
        "\n",
        "        return predicted_email, predicted_confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during inference for {audio_file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Reload the best saved model and mapping for inference\n",
        "loaded_model = SpeakerCNN(num_speakers=num_speakers, num_mfcc=train_dataset.num_mfcc).to(device) # Changed model class\n",
        "best_model_path = os.path.join(PROJECT_ROOT_DIR, 'saved_models_cnn_only', 'speaker_cnn_best.pth') # Changed path/name\n",
        "model_final_save_path = os.path.join(PROJECT_ROOT_DIR, 'saved_models_cnn_only', 'speaker_cnn_final.pth') # Changed path/name\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    loaded_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    print(f\"Loaded best model from: {best_model_path}\")\n",
        "else:\n",
        "    loaded_model.load_state_dict(torch.load(model_final_save_path, map_location=device))\n",
        "    print(f\"Loaded final model from: {model_final_save_path} (Best model not found)\")\n",
        "\n",
        "loaded_speaker_mapping = joblib.load(os.path.join(PROJECT_ROOT_DIR, 'saved_models_cnn_only', 'speaker_mapping_cnn_only.joblib')) # Changed mapping name\n",
        "\n",
        "# Load existing_files_df to get names\n",
        "try:\n",
        "    full_df = pd.read_csv(CSV_PATH)\n",
        "    full_df['audio_path'] = full_df['audio_file'].apply(lambda x: os.path.join(AUDIO_FOLDER_PATH, x))\n",
        "    existing_files_df_temp = full_df[full_df['audio_path'].apply(os.path.exists)].copy().reset_index(drop=True)\n",
        "    existing_files_df_temp['speaker_id'] = existing_files_df_temp['email'].astype('category').cat.codes\n",
        "    existing_files_df = existing_files_df_temp\n",
        "    print(\"Reloaded existing_files_df for name lookup.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reloading existing_files_df for name lookup: {e}\")\n",
        "    existing_files_df = pd.DataFrame({'email': [], 'name': []})\n",
        "\n",
        "\n",
        "print(\"\\n--- Upload an audio file from your PC for speaker detection ---\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "if uploaded_files:\n",
        "    uploaded_file_name = list(uploaded_files.keys())[0]\n",
        "    uploaded_file_path = os.path.join('/content/', uploaded_file_name)\n",
        "\n",
        "    print(f\"\\nUploaded file: {uploaded_file_name}\")\n",
        "    print(f\"File saved to: {uploaded_file_path}\")\n",
        "\n",
        "    print(f\"\\n--- Performing Inference on the uploaded audio file ---\")\n",
        "\n",
        "    detected_email, confidence = predict_speaker_from_audio(\n",
        "        loaded_model, uploaded_file_path, loaded_speaker_mapping,\n",
        "        num_mfcc=train_dataset.num_mfcc\n",
        "    )\n",
        "\n",
        "    if detected_email:\n",
        "        print(\"\\n--- Detection Result ---\")\n",
        "        print(f\"Corresponding Email ID: {detected_email}\")\n",
        "        print(f\"Confidence: {confidence:.4f}\\n\")\n",
        "\n",
        "    else:\n",
        "        print(\"Detection failed for the uploaded file. No matching speaker found or an error occurred.\")\n",
        "        print(\"Ensure it's a clear recording of one of the trained speakers.\")\n",
        "        print(f\"Best confidence achieved (if any): {confidence:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"No file was uploaded.\")\n"
      ],
      "metadata": {
        "id": "zt7IZB8HgRXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}