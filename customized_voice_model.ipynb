{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9c/b6GW0bowY3JTLUTEQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sazim2019331087/voice_model/blob/main/customized_voice_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing necessary libraries"
      ],
      "metadata": {
        "id": "JOMFUVgnCjFl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZbKzy_g6r5z",
        "outputId": "b46fd5f8-a589-419e-cfeb-b7bb48519dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchaudio pandas scikit-learn pydub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "h7B1yJfFDSf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pydub import AudioSegment\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "q6po-ECZDVBn"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Speaker Details"
      ],
      "metadata": {
        "id": "nFR91qOIE2OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = \"main_data.csv\"\n",
        "df = pd.read_csv(csv_file)"
      ],
      "metadata": {
        "id": "F3ezkmO4E46z"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting MFCC features"
      ],
      "metadata": {
        "id": "08bjMMCHDW5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def extract_mfcc(file_path, n_mfcc=13, n_mels=40, fixed_length=100):\n",
        "    try:\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "        # Convert to mono if multi-channel\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Handle empty audio files\n",
        "        if waveform.shape[1] == 0:\n",
        "            print(f\"Warning: Empty audio file {file_path}\")\n",
        "            return torch.zeros(n_mfcc, fixed_length)\n",
        "\n",
        "        # Extract MFCC features\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=n_mfcc,\n",
        "            melkwargs={'n_mels': n_mels}\n",
        "        )(waveform)\n",
        "\n",
        "        # Convert shape from (1, n_mfcc, time_steps) â†’ (n_mfcc, time_steps)\n",
        "        mfcc = mfcc.squeeze(0)\n",
        "\n",
        "        # Fix shape: Padding or truncation\n",
        "        if mfcc.shape[1] < fixed_length:\n",
        "            mfcc = F.pad(mfcc, (0, fixed_length - mfcc.shape[1]), \"constant\", 0)\n",
        "        else:\n",
        "            mfcc = mfcc[:, :fixed_length]\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return torch.zeros(n_mfcc, fixed_length)  # Return zeros if error occurs"
      ],
      "metadata": {
        "id": "CjqIz0HaEEQZ"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Label Encoding for speakers"
      ],
      "metadata": {
        "id": "iBpdn0oPEGkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "df['speaker_id'] = label_encoder.fit_transform(df['email'])"
      ],
      "metadata": {
        "id": "6QrdkqIbEq3t"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Label Encodings"
      ],
      "metadata": {
        "id": "ql5o7Lz5FCpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(label_encoder, \"label_encoder.pth\")"
      ],
      "metadata": {
        "id": "W5OmVbeBFEyY"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Custom Dataset of Speakers"
      ],
      "metadata": {
        "id": "HgavuHe8FIEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeakerDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "        self.labels = torch.tensor(dataframe['speaker_id'].values, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_file = os.path.join(\"voices\", self.dataframe.iloc[idx][\"audio_file\"])  # Fix column name\n",
        "        mfcc = extract_mfcc(audio_file)  # Extract MFCC features\n",
        "        return mfcc, self.labels[idx]  # Do not flatten, let collate_batch handle padding"
      ],
      "metadata": {
        "id": "LuDu18c2FPx-"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation of dataset"
      ],
      "metadata": {
        "id": "eYQFPQ3kFnfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SpeakerDataset(df)"
      ],
      "metadata": {
        "id": "nm8ToWOhFsh1"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spliting Dataset into Train & Test"
      ],
      "metadata": {
        "id": "RCgiP9IuFxeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
      ],
      "metadata": {
        "id": "hRdt_R8BF001"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Custom Collating Batch for Padding"
      ],
      "metadata": {
        "id": "QWezQGP2PMoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "    features, labels = zip(*batch)\n",
        "\n",
        "    # Convert features to tensors and pad sequences\n",
        "    features = [torch.tensor(f, dtype=torch.float32) for f in features]\n",
        "    features_padded = pad_sequence(features, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Flatten the feature dimension to match model input (batch_size, time_steps * n_mfcc)\n",
        "    features_padded = features_padded.view(features_padded.shape[0], -1)\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return features_padded, labels"
      ],
      "metadata": {
        "id": "CGiNMJJoPS2W"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Data Loaders"
      ],
      "metadata": {
        "id": "ocign_GRF5RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "vzl92F41F9tB"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_features, _ = next(iter(train_loader))  # Get a batch of data\n",
        "input_size = sample_features.shape[-1]  # Extract feature dimension (MFCC coefficients)\n",
        "num_classes = len(df[\"speaker_id\"].unique())  # Get number of unique speakers\n",
        "print(f\"Updated input_size: {input_size}, Number of classes: {num_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBQbJknsRHgR",
        "outputId": "ad80602a-ecf8-48ec-a13c-4647e6cb4841"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated input_size: 1300, Number of classes: 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-114-eadfafccbc61>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  features = [torch.tensor(f, dtype=torch.float32) for f in features]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Neural Network Model"
      ],
      "metadata": {
        "id": "PLLF6N2tF_J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SpeakerClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(SpeakerClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "p9BFnuIbGE-2"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Input Size from sample"
      ],
      "metadata": {
        "id": "CBk-NHbVGHoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_mfcc, _ = dataset[0]\n",
        "input_size = sample_mfcc.numel()  # Fix input_size calculation\n",
        "num_classes = len(df['speaker_id'].unique())"
      ],
      "metadata": {
        "id": "1v3DsjXTGTct"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "_QHwRx8TGVWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SpeakerClassifier(input_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for features, labels in train_loader:\n",
        "        features = features.to(device).float()\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "\n",
        "        # Debugging Check\n",
        "        print(f\"Output Shape: {outputs.shape}, Label Shape: {labels.shape}\")\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5DSC5rMGdgI",
        "outputId": "ec277e54-0134-46f9-db15-482b669d0daa"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-114-eadfafccbc61>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  features = [torch.tensor(f, dtype=torch.float32) for f in features]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [1/30], Loss: 44.9646\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [2/30], Loss: 50.4056\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [3/30], Loss: 37.4692\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [4/30], Loss: 32.3845\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [5/30], Loss: 27.2090\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [6/30], Loss: 19.6908\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [7/30], Loss: 16.0204\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [8/30], Loss: 12.5201\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [9/30], Loss: 10.9184\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [10/30], Loss: 9.6930\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [11/30], Loss: 8.5931\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [12/30], Loss: 6.9974\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [13/30], Loss: 5.0984\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [14/30], Loss: 4.7002\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [15/30], Loss: 4.5519\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [16/30], Loss: 3.4698\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [17/30], Loss: 2.3821\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [18/30], Loss: 1.9225\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [19/30], Loss: 1.4443\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [20/30], Loss: 1.3266\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [21/30], Loss: 1.2130\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [22/30], Loss: 1.1394\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [23/30], Loss: 1.2308\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [24/30], Loss: 1.1744\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [25/30], Loss: 0.9489\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [26/30], Loss: 0.8019\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [27/30], Loss: 0.7262\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [28/30], Loss: 0.6596\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [29/30], Loss: 0.6000\n",
            "Output Shape: torch.Size([30, 38]), Label Shape: torch.Size([30])\n",
            "Epoch [30/30], Loss: 0.5508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the trained Model"
      ],
      "metadata": {
        "id": "gRO9Wq5XLbkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where \"main_data.csv\" is stored\n",
        "DATA_PATH = \"/content\"  # Same directory as main_data.csv\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "model_path = os.path.join(DATA_PATH, 'speaker_classifier.pth')\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Load model (if needed)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.to(device)\n",
        "#model.eval()\n",
        "\n",
        "print(f\"Model saved at: {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAhM82JXLd2d",
        "outputId": "7299f1ae-e9a6-4685-f9e8-343541f53c92"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved at: /content/speaker_classifier.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-130-f5e41856620c>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "iCd0TTIbLiRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        features = torch.stack([torch.tensor(f, dtype=torch.float32) for f in features]).to(device)\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv6MyJvLLni-",
        "outputId": "95dc69b1-a0f0-4b9a-d21f-760fc6605aca"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-114-eadfafccbc61>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  features = [torch.tensor(f, dtype=torch.float32) for f in features]\n",
            "<ipython-input-131-daf555c21fae>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  features = torch.stack([torch.tensor(f, dtype=torch.float32) for f in features]).to(device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speaker Predictions"
      ],
      "metadata": {
        "id": "dpMWTiLdf44z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_speaker(file_path, model, label_encoder, fixed_length=1300, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    model.to(device)\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    try:\n",
        "        # Extract MFCC using your existing function\n",
        "        mfcc = extract_mfcc(file_path).to(device)  # Move to correct device\n",
        "\n",
        "        # Flatten MFCC to match model input\n",
        "        mfcc = mfcc.view(1, -1)  # Shape: (1, feature_size)\n",
        "\n",
        "        # Ensure fixed input size (truncate or pad)\n",
        "        current_size = mfcc.shape[1]\n",
        "        if current_size < fixed_length:\n",
        "            pad_size = fixed_length - current_size\n",
        "            mfcc = F.pad(mfcc, (0, pad_size), \"constant\", 0)  # Pad with zeros\n",
        "        else:\n",
        "            mfcc = mfcc[:, :fixed_length]  # Truncate\n",
        "\n",
        "        # Predict speaker\n",
        "        with torch.no_grad():\n",
        "            output = model(mfcc)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "\n",
        "        # Decode predicted label\n",
        "        speaker_email = label_encoder.inverse_transform([predicted.item()])[0]\n",
        "\n",
        "        print(f\"Predicted Speaker: {speaker_email}\")\n",
        "        return speaker_email\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error predicting speaker: {e}\")\n",
        "        return None  # Return None if prediction fails\n"
      ],
      "metadata": {
        "id": "xwlC31j6f6uc"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "T55kUYK2gMyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query_file = os.path.join(\"/content\", \"6759b4871227a.wav\")  # Ensure correct path\n",
        "predict_speaker(\"/content/voices/6759b4871227a.wav\", model, label_encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "RBc-8QqqgOEE",
        "outputId": "15df7cde-7456-41c3-e73e-c62ee7a61c3a"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Speaker: royt26850@gmail.com\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'royt26850@gmail.com'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    }
  ]
}